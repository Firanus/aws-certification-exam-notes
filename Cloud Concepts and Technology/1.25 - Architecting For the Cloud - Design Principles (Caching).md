# 1.25 - Architecting For the Cloud - Design Principles (Caching)

Caching is a technique for storing previously calculated data to for future use, in order to improve the performance and cost efficiency of an application. It can appear at multiple layers in an application. Here, we'll look at two types of caching.

## Application Data Caching

Applications can be designed so that they store and retreive information from fast, managed, in-memory caches.

In general, when requesting information (e.g. the results of I/O intensive DB queries, or the outcome of computationally expensive processing) the application will first check the cache. If it cannot find the information there, it will either retrieve or calculate the info and store it in the cache for future use.

The advantage of this is that on subsequent retrivels of the data, the app can use the data directly, which reduces latency to your end users, and reduces the load on backend systems.

How long you cache data for is down to your application. Mileage will vary.

Amazon's main service for caching is **ElastiCache**. It, in turn, supports two open-source, in-memory caching engines, Memcached and Redis. For more detail, see the [Performance at Scale with Amazon ElastiCache](https://d0.awsstatic.com/whitepapers/performance-at-scale-with-amazon-elasticache.pdf) whitepaper.

In addition Amazon also provides a cache for DynamoDB, called DynamoDB Accelerator (DAX).

## Edge Caching

In addition to in-memory caching, Amazon also allows you to store copies of both static (images, CSS files, pre-recorded video) and dynamic (responsive HTML, live video) content at Amazon CloudFront Edge Locations. CloudFront is a CDN, and using its edge caching features allow content to be served to users from infrastructure closer to them, which reduces latency and hence improves data transfer rates.