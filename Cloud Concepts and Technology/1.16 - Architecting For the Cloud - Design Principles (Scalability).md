# 1.16 - Architecting For the Cloud - Design Principles (Scalability)

Scalability refers to an application's ability to grow (and shrink) over time to handle changes in usage, traffic and data size. In general, that growth should also take advantage of economies of scale. There are two main ways to scale an application, vertically and horizontally.

## Vertically

This is the traditional equivalent of buying a bigger server. In AWS, this involves taking an instance down, and resizing it to have more RAM, CPU, I/O or networking capabilities as necessary.

Vertical scaling isn't always cost-efficient or highly available, and it has limits. However, it is very easy to implement, and can be sufficient for many use cases, especially short term ones.

## Horizontally

This is the traditional equivalent of buying a second server. This is an excellent way to build internet-scale applications that leverage the elasticity of the cloud. However, this process is more complex, and not all applications are designed to handle it. Let's look at some examples.

### Stateless Applications

A stateless application is essentially an algorithm in the cloud. It has no memory of any previous interaction with it, and keeps no track of session information. (For this reason, if you make the same request to a stateless application, it will always return the same result).

Stateless applications are very easy to scale horizontally, because any of the available compute services (e.g. EC2 instances or Lambda functions) can service the request. To increase your load, you just add an instance and tell your load balancer to send requests to it. Decreasing it is a matter or just removing it. Nothing you do to that instance will affect the others, so the process is pretty safe.

### Distribute Load to Multiple Nodes

To distribute the workload to multiple nodes, you can choose either a *push* or a *pull* model.

In a *push* model, you essentially use an Elastic Load Balancer to distribute incoming requests amongst multiple EC2 instances. An alternative is to use Route 53 to implement a DNS Round Robin, where each request gets sent to a different IP address, which corresponds to a different instance. This is less reliable due to the unpredictable behaviour of caching DNS resolvers.

Instead of a push model, if you have an asynchronous, event-driven workload, you can implement a *pull* model. In this model, tasks that need doing are stored as messages in a queue of some sort (e.g. Amazon Simple Queue Service, or Amazon Kinesis for streaming data). Multiple compute services can then pull and consume those messages in a distributed fashion.

### Stateless Components

In practice, most applications maintain some kind of state information. You can still make a portion of these architectures stateless by not storing anything that needs persist longer than a single request in the local file system.

An example is HTTP session cookies. Rather than storing these locally on the browser, it makes sense to pass them back to the server or a database of some kind where they can be stored, and the browser application can say stateless. Note that there are multiple drawbacks here (e.g. the ability for the client ot taper with the cookie, the overhead of passing it back and forth) which need to be handled sensibly.

#### Stateful Components

Inevitably, layers of your architecture will need to be stateful, e.g. all your datatbases. This is also the case for many legacy applications, and specific use cases, like real time multiplayer games, which require multiple users to have a consistent view of the game world with very low latency.

There is still potential for horizontally scaling out here, e.g. with read-replicas of a database, or by distributing to nodes with session affinity. In this case, you'd bind all the transactions of a session to a specific compute resource. There are limitations though; existing sessions don't benefit from scaling up compute resources, and terminating an instance will result in the loss of session specific data.

### Implementing Session Affinity

For HTTP and HTTPS traffic, your Application Load Balancer can use sticky session features to try to bind a user's session to a specific instance for the duration of the session.

Another option - if you control the client-side code - is to implement client-side load balancing. This adds complexity, but is useful in situations where a load balancer doesn't meet your requirements, e.g. you need full control over how users are assigned to servers.

### Distributed Processing

In a data context, if you need to process more data than can reasonable handled by a single compute instance, what's necessary is to distribute the processing. Commonly, this is done by finding ways to divide the task and its data into small fragments of work that can be executed in parallel across multiple compute resources

Services that support this include AWS Batch, AWS Glue and Apache Hadoop. Amazon EMR can run Hadoop workloads in top of a fleet of EC2 instances. Amazon Kinesis can partition data into multiple shards for consumption by Lambda functions or EC2 instances.