# 1.23 - Architecting For the Cloud - Design Principles (Remove Single Points of Failure)

Produciton systems almost always come with either explicit or implicit objectives for uptime. A system is only **highly available** if it can withstand the failure of one or more of its components, including hard disks, servers, and network links.

To build systems with high availability, we'll look at a number of ways we can automate recovery and reduce disruption at every level of our architecture.

For more info, see Amazon's [Building Fault Tolerant Applications](https://d0.awsstatic.com/whitepapers/aws-building-fault-tolerant-applications.pdf) whitepaper

## Introduce Redundancy

One way of removing single points of failure is to have multiple resources for the same task. This is called redundancy. There are two ways to implement it, in standby mode and in active mode.

### Standby Redundancy

In standby redundancy, when a resource fails, functionality is recovered by a secondary resource with the **failover** process. The failover typically requires some time to complete, during which your resource is unavailable. A secondary resource can either:
* **Be launched when needed** - This reduces cost but increases failover time
* **Be running idle constantly** - This reduces failover time, and hence user disruption, but at a cost.

Standby redundancy is typically used for stateful components like relational databases.

### Active Redundancy

In active redundancy, requests are distributed to multiple redundant compute resources. When one of them fails, the others simply absorb its share of the workload.

Compared to standby redundancy, active redundancy typically achieves better usage, and affects a smaller population when there is a failure. It is most suitable for stateless components.

## Detect Failure

A key step of being able to design for failure is being able to detect it in the first place. Services such as ELB and Route 53 can be configured to run health checks and mask failure by routing traffic to healthy endpoints.

In addition, features like auto-scaling, EC2's auto-recover feature, or services like AWS Elastic Beanstalk can be configured to automatically replace unhealthy nodes.

You won't be able to anticipate all your failures on Day 1. Instead, recognize this, and make sure you're collecting enough logs and metrics to be able to understand the behaviour of your system. As you grow to understand it, you'll be ble to set up appropriate alarms for manual and automated intervention.

### Designing Good Health Checks

There is a bit of an art to designing good health checks. As with all software design, the key is to keep your use case in mind, and build appropriately. We'll provide two example before.

For health checks on your ELB, its important that you reliably assess the health of your backend endpoints, e.g. by ensuring the return a 200 response for a simple request. However, you might want to avoid doing checks that go "deep" into your stack here, as they will make it harder to find problems; if a DB instance fails and but the ELB registers the service down, it makes it harder to debug.

On the other hand, such "E2E" checks can make sense at the Route 53 level, where if any part of the stack fails, you want to be able to direct your users to a backup, static version of your website.

Be practical of your designs, and always be willing to be flexible; each use case is a little different.

## Durable Data Storage

Your application and users are going to store data. For almost all applications, it's crucial that your architecture protects data availability and data integrity. The most common way to do this is to replicate your data to introduce redundancies. There are a number of ways to go about this.

### Synchronous Replication

Synchronous replication only acknowledges a transaction after its been stored in both the primary location and its replicas. It's ideal for ensuring integrity if your primary node fails. It can also scale read-capacity for results that require the most up-to-date data.

The drawbacks are that it couples your primary node to its replicas. In topologies that run across unreliable or high-latency networks, this can compromise performance and availability.

In addition, regardless of how durable the solution is, backups are vital. Enabling versioning (particularly for objects in S3) can make it much easier to recover from both application errors and unintended user actions.

### Asynchronous Replication

This mode decouples the primary node from its replicas at the cost of introducing replication lag. This means that read queries might not always have the most up-to-date information. Consider it in cases where some loss of recent transactions can be tolerated over a failover, for example, maintaining an autonomous replica of a database in another region for disaster recovery.

**Quorum-based replication** is a combination of both the previous approaches.

## Automating Multi-Data Center Resilience

In essence this section boils down to a describing how AZs and regions function.

Truly global businesses need to be able failures on a larger scale than those that might affect a single server. Truly global business should be able to handle natural disasters.

In AWS, the standard way to handle these situations would be to replicate your data across multiple regions. This solution is slightly larger scale than is discussed in the whitepaper. Luckily, there is another whitepaper which covers exactly that topic: [AWS Disaster Recover](https://media.amazonwebservices.com/AWS_Disaster_Recovery.pdf)

On a smaller scale (e.g. a data center is unavailable for a short period of time), the solution is to replicate your data across multiple availability zones.

Many of the higher level services on AWS are inherently designed according to the multi-AZ principle, including Amazon RDS. Amazon S3 and DynamoDB offer similar functionality.

## Fault Isolation and Traditional Horizontal Scaling

Though active redundancy solves many problems, there is a situation it doesn't account for: harmful requests. If there is a particular request that causes a system to fail over, the caller could trigger a cascading failure by repeatedly trying the same request against all the instances.

A solution to this problem is known as **shuffle sharding**. In essence, this involves breaking your instances into groups called shards (e.g. 8 instances could become 4 shards with two instances each). You then distribute your customers to a specific shard.

For more information about this technique, have a look at the [Shuffle Sharding blog post](https://aws.amazon.com/blogs/architecture/shuffle-sharding-massive-and-magical-fault-isolation/)